# -*- coding: utf-8 -*-
"""Smilegate-AI UnSmile dataset fine-tuning tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NKYYVSex__vde-lnYCmsRmyHjJhV6cKt

# Smilegate UnSmile Dataset Tutorial

[Smilegate AI 센터](https://smilegate.ai) 에서 공개하는 한국어 혐오표현 "☹️ [UnSmile](https://github.com/smilegate-ai/korean_unsmile_dataset)" 데이터셋 사용을 위한 tutorial 실습 노트북입니다.

> Contact      
```
* 김성현 (seonghkim@smilegate.com)  
```

Smilegate AI `UnSmile`의 `소스코드 및 baseline 모델`은 [Apache 2.0](LICENSE.apache-2.0) 라이선스 하에 공개되어 있습니다.

# 0. 환경설정

본 실습에서는 Huggingface [Transformers](https://github.com/huggingface/transformers)와 [Datasets](https://github.com/huggingface/datasets) 라이브러리를 사용합니다.
"""

# !pip install transformers
# !pip install datasets==1.17.0

"""# 1. 데이터셋 load

Huggingface Datasets를 통해 데이터를 load해보겠습니다.
"""

from datasets import load_dataset

dataset = load_dataset('smilegate-ai/kor_unsmile')

dataset["train"][0]

unsmile_labels = ["여성/가족","남성","성소수자","인종/국적","연령","지역","종교","기타 혐오","악플/욕설","clean"]
# 개인지칭의 경우, 추가 정보이므로 분류 대상에서 제외했습니다.

"""# 2. Model load

학습을 위해 Pretrained language model (PLM) 을 활용해보겠습니다.
"""

from transformers import BertForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer
import torch
import numpy as np

model_name = 'beomi/kcbert-base'

tokenizer = AutoTokenizer.from_pretrained(model_name)

"""bert model에 학습 데이터 전달을 위해 tokenizing 작업을 수행합니다."""

def preprocess_function(examples):
    tokenized_examples = tokenizer(str(examples["문장"]))
    tokenized_examples['labels'] = torch.tensor(examples["labels"], dtype=torch.float)
    # multi label classification 학습을 위해선 label이 float 형태로 변형되어야 합니다.
    # huggingface datasets 최신 버전에는 'map' 함수에 버그가 있어서 변형이 올바르게 되지 않습니다.
    
    return tokenized_examples

tokenized_dataset = dataset.map(preprocess_function)
tokenized_dataset.set_format(type='torch', columns=['input_ids', 'labels', 'attention_mask', 'token_type_ids'])

tokenized_dataset['train'][0]

from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

num_labels=len(unsmile_labels) # Label 갯수

model = BertForSequenceClassification.from_pretrained(
    model_name, 
    num_labels=num_labels, 
    problem_type="multi_label_classification"
)
model.config.id2label = {i: label for i, label in zip(range(num_labels), unsmile_labels)}
model.config.label2id = {label: i for i, label in zip(range(num_labels), unsmile_labels)}

model.config.label2id

"""# 3. Model training"""

# from sklearn.metrics import label_ranking_average_precision_score
#
# def compute_metrics(x):
#     return {
#         'lrap': label_ranking_average_precision_score(x.label_ids, x.predictions),
#     }
#
# batch_size = 64 # 64 batch는 colab pro에서 테스트되었습니다.
#
# args = TrainingArguments(
#     output_dir="model_output",
#     evaluation_strategy="epoch",
#     learning_rate=2e-5,
#     per_device_train_batch_size=batch_size,
#     per_device_eval_batch_size=batch_size,
#     num_train_epochs=5,
#     save_strategy='epoch',
#     load_best_model_at_end=True,
#     metric_for_best_model='lrap',
#     greater_is_better=True,
# )
#
# trainer = Trainer(
#     model=model,
#     args=args,
#     train_dataset=tokenized_dataset["train"],
#     eval_dataset=tokenized_dataset["valid"],
#     compute_metrics=compute_metrics,
#     tokenizer=tokenizer,
#     data_collator=data_collator
# )
#
# trainer.train()
#
# trainer.save_model()

"""# 4. Model test

직접 학습하신 모델을 사용하실 경우, 아래 코드로 실행해주세요
"""

# from transformers import TextClassificationPipeline
#
# pipe = TextClassificationPipeline(
#     model = model,
#     tokenizer = tokenizer,
#     device=0,
#     return_all_scores=True,
#     function_to_apply='sigmoid'
#     )

"""기학습된 모델을 사용하실 경우, 아래 코드로 실행해주세요"""

from transformers import TextClassificationPipeline, BertForSequenceClassification, AutoTokenizer

model_name = 'smilegate-ai/kor_unsmile'

model = BertForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

from flask import Flask, jsonify, request
from flask_cors import CORS

app = Flask(__name__)
CORS(app)
app.config['JSON_AS_ASCII'] = False



@app.route('/comment', methods=['POST'])
def commentCheck():
    
    if request.method == 'POST':
        comment = request.form['comment']
    
        pipe = TextClassificationPipeline(
            model=model,
            tokenizer=tokenizer,
            device=-1,     # cpu: -1, gpu: gpu number
            return_all_scores=True,
            function_to_apply='sigmoid'
            )
        
        resultList = []
        maxComment = 0
        maxidx = 0
        maxLabel = ''
        for result in pipe(comment)[0]:
            resultList.append(result)
        
        for idx, val in enumerate(resultList):
            if maxComment < val['score']:
                maxidx = idx
                maxComment = val['score']
                maxLabel = val['label']
        if (maxLabel != 'clean' and maxComment < 0.5):
            maxLabel = 'clean'

        
        
        return {'commentResult': maxLabel}

@app.route('/article', methods=['POST'])
def articleCheck():
    if request.method == 'POST':
        title = request.form['title']
        content = request.form['content']
      
        pipe = TextClassificationPipeline(
            model=model,
            tokenizer=tokenizer,
            device=-1,     # cpu: -1, gpu: gpu number
            return_all_scores=True,
            function_to_apply='sigmoid'
            )
        
        titleList = []
        contentList = []
        for result in pipe(title)[0]:
            titleList.append(result)

        for result in pipe(content)[0]:
            contentList.append(result)
        
        maxTitle = 0
        maxTitleLabel = '' 
        for idx, val in enumerate(titleList):
            if maxTitle < val['score']:
                maxTitle = val['score']
                maxTitleLabel = val['label']

        if (maxTitleLabel != 'clean' and maxTitle < 0.5):
            maxTitleLabel = 'clean'

        maxContent = 0
        maxContentLabel = '' 
        for idx, val in enumerate(contentList):
            if maxContent < val['score']:
                maxContent = val['score']
                maxContentLabel = val['label']
        
        if (maxContentLabel != 'clean' and maxContent < 0.5):
            maxContentLabel = 'clean'
            
        
        return {'contentResult': maxContentLabel, 'titleResuit': maxTitleLabel}

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8081, debug=True)



"""# 5. Model evaluation"""
#
# def get_predicated_label(output_labels, min_score):
#     labels = []
#     for label in output_labels:
#         if label['score'] > min_score:
#             labels.append(1)
#         else:
#             labels.append(0)
#     return labels
#
# import tqdm
# from transformers.pipelines.base import KeyDataset
#
# predicated_labels = []
#
# for out in tqdm.tqdm(pipe(KeyDataset(dataset['valid'], '문장'))):
#     predicated_labels.append(get_predicated_label(out, 0.5))
#
# from sklearn.metrics import classification_report
#
# print(classification_report(dataset['valid']['labels'], predicated_labels))
