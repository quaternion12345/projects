{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgCoVX-1JJC4"
      },
      "source": [
        "Mnist Classifier using Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeVGWZGKy0Yn"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn.init\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.datasets as datasets\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import numpy as np\r\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9WaaqRYJUA7"
      },
      "source": [
        "1. Define Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVx9I7A_zVHU"
      },
      "source": [
        "# Multi Layer Perceptron\r\n",
        "class Net(nn.Module):\r\n",
        "  \r\n",
        "  def __init__(self):\r\n",
        "    super(Net, self).__init__()\r\n",
        "    self.l1 = nn.Linear(784,520)\r\n",
        "    self.l2 = nn.Linear(520,320)\r\n",
        "    self.l3 = nn.Linear(320,240)\r\n",
        "    self.l4 = nn.Linear(240,120)\r\n",
        "    self.l5 = nn.Linear(120,10)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = x.view(-1,784) # Flatten data\r\n",
        "    # Neural Network\r\n",
        "    x = F.relu(self.l1(x))\r\n",
        "    x = F.relu(self.l2(x))\r\n",
        "    x = F.relu(self.l3(x))\r\n",
        "    x = F.relu(self.l4(x))\r\n",
        "    return self.l5(x)\r\n",
        "\r\n",
        "net = Net()\r\n",
        "\r\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6zq-iUOJfH_"
      },
      "source": [
        "2. Define Hyperparameters, Loss function, Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nyyzwmj6TWF"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "# Set Hyperparameter\r\n",
        "learning_rate = 0.001\r\n",
        "training_epochs = 20\r\n",
        "batch_size = 100\r\n",
        "valid_size = 0.2\r\n",
        "\r\n",
        "# Set Loss, Optimizer\r\n",
        "criterion = nn.CrossEntropyLoss() # softmax + cross entropy\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate) # ADAM optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAcglOaEJqRm"
      },
      "source": [
        "3. Load Dataset for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C-8auHB9KTd"
      },
      "source": [
        "# Download datasets\r\n",
        "mnist_train = datasets.MNIST(root='data', # download location\r\n",
        "                          train=True,\r\n",
        "                          transform=transforms.ToTensor(), # convert to tensor\r\n",
        "                          download=True)\r\n",
        "\r\n",
        "mnist_test = datasets.MNIST(root='data',\r\n",
        "                         train=False, \r\n",
        "                         transform=transforms.ToTensor(),\r\n",
        "                         download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yzxn3TTJ3_K"
      },
      "source": [
        "4. Divide training dataset into training dataset and validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LwLXIST8vG6"
      },
      "source": [
        "# Prepare dataset for train, validation, test\r\n",
        "# divide data between train set and validation set\r\n",
        "num_train = len(mnist_train)\r\n",
        "indices = list(range(num_train))\r\n",
        "np.random.shuffle(indices)\r\n",
        "split = int(np.floor(valid_size * num_train))\r\n",
        "train_index, valid_index = indices[split:], indices[:split]\r\n",
        "\r\n",
        "# define sampler\r\n",
        "train_sampler = SubsetRandomSampler(train_index)\r\n",
        "valid_sampler = SubsetRandomSampler(valid_index)\r\n",
        "\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train, \r\n",
        "                                           batch_size = batch_size, \r\n",
        "                                           sampler = train_sampler)\r\n",
        "valid_loader = torch.utils.data.DataLoader(mnist_train, \r\n",
        "                                           batch_size = batch_size,\r\n",
        "                                           sampler = valid_sampler)\r\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, \r\n",
        "                                          batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3x6maVbKZ_7"
      },
      "source": [
        "5. Train and evaluate the model(Neural Network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hLqGGOd0adn"
      },
      "source": [
        "for epoch in range(training_epochs):\r\n",
        "    # monitor loss\r\n",
        "    train_loss = 0\r\n",
        "    valid_loss = 0\r\n",
        "    \r\n",
        "    # train the model   \r\n",
        "    net.train()\r\n",
        "    for data,label in train_loader:\r\n",
        "        # clear the gradients of all optimized variables\r\n",
        "        optimizer.zero_grad()\r\n",
        "        # forwarding inputs\r\n",
        "        output = net(data)\r\n",
        "        # calculate the loss\r\n",
        "        loss = criterion(output,label)\r\n",
        "        # backward propagation\r\n",
        "        loss.backward()\r\n",
        "        # update parameter\r\n",
        "        optimizer.step()\r\n",
        "        # update training loss\r\n",
        "        train_loss += loss.item() * data.size(0)\r\n",
        "            \r\n",
        "    # validate the model\r\n",
        "    net.eval()\r\n",
        "    for data,label in valid_loader:\r\n",
        "        # forwarding inputs\r\n",
        "        output = net(data)\r\n",
        "        # calculate the loss\r\n",
        "        loss = criterion(output,label)\r\n",
        "        # update validation loss \r\n",
        "        valid_loss = loss.item() * data.size(0)\r\n",
        "    \r\n",
        "    # print training/validation statistics \r\n",
        "    # calculate average loss over an epoch\r\n",
        "    train_loss = train_loss / len(train_loader.sampler)\r\n",
        "    valid_loss = valid_loss / len(valid_loader.sampler)\r\n",
        "    \r\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\r\n",
        "        epoch+1, \r\n",
        "        train_loss,\r\n",
        "        valid_loss\r\n",
        "        ))   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xICnsEU5KsRe"
      },
      "source": [
        "6. Test the model(Neural Network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxnSSwJs6-Bb"
      },
      "source": [
        "# initialize lists to monitor test loss and accuracy\r\n",
        "test_loss = 0.0\r\n",
        "# Make list for containing correct and total items for each class\r\n",
        "class_correct = list(0. for i in range(10))\r\n",
        "class_total = list(0. for i in range(10))\r\n",
        "\r\n",
        "net.eval()\r\n",
        "for data, target in test_loader:\r\n",
        "    # forwarding inputs\r\n",
        "    output = net(data)\r\n",
        "    # calculate the loss\r\n",
        "    loss = criterion(output, target)\r\n",
        "    # update test loss \r\n",
        "    test_loss += loss.item()*data.size(0)\r\n",
        "    # convert output probabilities to predicted class\r\n",
        "    _, pred = torch.max(output, 1)\r\n",
        "    # compare predictions to true label\r\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\r\n",
        "    # calculate test accuracy for each class\r\n",
        "    for i in range(len(target)):\r\n",
        "        label = target.data[i]\r\n",
        "        class_correct[label] += correct[i].item()\r\n",
        "        class_total[label] += 1\r\n",
        "        \r\n",
        "# print the test accuracy\r\n",
        "test_loss = test_loss/len(test_loader.sampler)\r\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\r\n",
        "for i in range(10):\r\n",
        "    if class_total[i] > 0:\r\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\r\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\r\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\r\n",
        "    else:\r\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\r\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\r\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\r\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}